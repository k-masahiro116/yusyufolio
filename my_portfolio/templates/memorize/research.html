{% extends 'memorize.html' %}
{% load static %}

{% block main %}
    <div class="back1">
        <h1 class="text-primary">Memorize: Research</h1>
    </div>
{% endblock %}

{% block content %}

    <div class="blog">
        <div class="center">
            <div class="c2">
                <h2><b>Research: Dialog Systemについて</b></h2>
                <div class="c3-2">
                    <div class="blog_block">
                        <h4><b>Dialog Systemとは</b></h4>
                            電気通信大学 沼尾研究室で作成された対話システム</br>
                            2人チーム(須崎, 河村)と沼尾先生の助力のもと、開発が行われました</br>
                        </br>
                        <h5><b>対話システム</b></h5>
                            対話システム、または会話エージェント(Conversational Agent: CA)は、人間と会話することを目的としたコンピュータシステムのことを差しています。ただし、チャットボット
                            (雑談対話システム)とは異なります。主に対話システムは、複数のコンポーネントから構成されています。</br>
                            </br>
                        <h5><b>コンポーネント</b></h5>
                            対話システムに含まれるコンポーネントのセット、およびそれらのコンポーネントが責任をどのように分割するかは、システムごとに異なります。
                            また、対話システムの主幹となる部分は、対話の状態と対話戦略を管理するコンポーネントである対話マネージャー(Dialog Manager: DM)になります。
                            対話システムの典型的な活動サイクルには、以下のフェーズが含まれます。</br>
                            </br>
                            <ol>
                                <li>ユーザが発話すると、入力としてシステムの入力認識機能/デコーダーによってプレーンテキストに変換される(以下例)</li>
                                    <ul>
                                        <li>自動音声認識装置(ASR)</li>
                                        <li>ジェスチャー認識</li>
                                        <li>手書き認識</li>
                                    </ul>
                                <li>テキストは、自然言語理解(NLU)ユニットによって分析される(以下例)</li>
                                    <ul>
                                        <li>固有名の識別</li>
                                        <li>音声部分のタグづけ</li>
                                        <li>構文/意味パーサー</li>
                                    </ul>
                                <li>意味情報(セマンティック情報)は、DMによって分析される。DMは、ダイアログの履歴と状態を保持し、会話の一般的なフローを管理する。</li>
                                <li>通常、DMは、特定のタスクドメインを知っている1つ以上のタスクマネージャーに接続している</li>
                                <li>DMは、出力ジェネレータを使用して出力を生成する(以下例)</li>
                                    <ul>
                                        <li>自然言語生成</li>
                                        <li>ジェスチャー生成</li>
                                        <li>レイアウト マネージャー</li>
                                    </ul>
                                <li>最後に、出力は出力レンダラーを使用してレンダリングされる(以下例)</li>
                                    <ul>
                                        <li>テキスト読み上げエンジン(TTS)</li>
                                        <li>Computer facial animation</li>
                                        <li>ロボットまたはアバター</li>
                                    </ul>
                            </ol>
                            テキストのみのインターフェース(テキストベースのチャットなど)に基づく対話システムには、ステージ2~5のみが含まれます。</br>
                            </br>

                        <h5><b>Dialog Systemの構成</b></h5>
                            <img src=" {% static 'assets/img/dialog_system2.png'%} " width="70%" height="70%" border="0" style="margin:20px 20px 20px 20px">
                            </br>
                            主要コンポーネントは対話状態や対話戦略を管理する対話管理(DM), 自然言語生成(NLG), 音声処理や画像認識などマルチモーダルな情報を受け取る
                            Interface, マルチモーダル情報からSlotを認識する自然言語認識(NLU)の4つになる.</br>
                            </br>
                        <h4><b>より自然な対話システムを目指して</b></h4>
                            "自然対話システムは、人間の行動を模倣することによってユーザビリティとユーザー満足度を向上させようとする対話システムの形式である“(Berg, 2014)
                            これは、人間同士の対話の機能(サブ対話やトピックの変更など)に対応し、人間と機械の相互作用のための対話システムにそれらを統合することを目的としています。
                            多くの場合、(音声による)対話システムでは、非常に限られた語彙しか理解できません。また、トピックの変更に対応できず、ユーザが対話の流れに
                            影響を与えることができないため、ユーザがシステムに適応する必要性が生じてしまいます。そこで、ユーザが質問に答えるだけでなく、
                            対話に積極的に参加できるようにする方法として、混合イニシアチブというものがあります。しかし、混合イニシアチブの存在だけでは、自然な対話システム
                            として分類されるには不十分である。その他の重要な側面としては以下のようなものがある。</br>
                            </br>
                            <ul>
                                <li>システムの適応性</li>
                                <li>暗黙の確認のサポート</li>
                                <li>検証質問の使用</li>
                                <li>既に提供されている情報を修正する可能性</li>
                                <li>情報過多(求められているよりも多くの情報を提供する)</li>
                                <li>否定をサポートする</li>
                                <li>談話と照応を分析することによって参照を理解する</li>
                                <li>単調で繰り返し発生するプロンプトを防ぐための自然言語生成</li>
                                <li>適応型で状況を意識した定式化</li>
                                <li>社会的行動(挨拶, ユーザと同じレベルの形式, 礼儀正しさ)</li>
                                <li>音声認識と合成の品質</li>
                            </ul>
                            </br>
                            これらの側面のほとんどは多くの異なる研究プロジェクトの問題ですが、これらのトピックに対処する対話システムの開発をサポートする
                            ツールが不足しているのが現状です。また、業界の多くの音声対話システムの基盤であるVoiceXMLとALICEチャットボット
                            で有名なAIMLを除いて、これらのいずれも対話行為や言語などの言語機能を統合していません。(記述中)</br>
                            </br>
                    </div>
                    
                </div>
            </div>
        </div>
    </div>
    <style>
        body {
            margin: 40px 10px;
            padding: 0;
            font-family: Arial, Helvetica Neue, Helvetica, sans-serif;
        }
        blog{
            text-align: center;
        }
        .ul1 {
            border: 2px skyblue dashed;
        }
        ul {
            margin-bottom: 30px;
        }
        .stg2 {
            font-family: 'Vollkorn', serif;
        }
        #calendar {
            max-width: 1100px;
            margin: 0 auto;
        }
    </style>
{% endblock %}


